AI Safety Research
├── Outer Alignment
│   ├── Human preference learning (RLAIF, RLHF, CIRL)
│   ├── Reward modeling
│   └── Scalable oversight (Debate, RRH, Approval-based agents)
├── Inner Alignment
│   ├── Mesa-optimization
│   ├── Deception & honesty (TruthfulQA, latent knowledge)
│   └── Interpretability (SAEs, circuit tracing, ELK)
├── Robustness
│   ├── Adversarial examples
│   ├── Distribution shift
│   └── Goal misgeneralization
├── Value Alignment
│   ├── Ethical modeling (e.g., virtue RL, moral philosophy grounding)
│   ├── Cooperative AI & multi-agent ethics
│   └── Reflective agency, recursive reward modeling
├── Governance & Deployment
│   ├── Info hazards, model release protocols
│   ├── Alignment strategies under compute asymmetry
│   └── Global coordination
