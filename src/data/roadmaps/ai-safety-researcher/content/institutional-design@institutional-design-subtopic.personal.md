# Safety Institutions Design: Building Organizations That Don't Suck

Let me tell you about the time I watched a room full of brilliant people design the perfect AI safety institution. It had everything: elegant governance structures, balanced stakeholder representation, adaptive mechanisms, accountability frameworks. It was beautiful. It was comprehensive. It would have been completely dysfunctional.

Why? Because they forgot that institutions are made of humans, and humans are weird.

## My Journey from Org Charts to Organized Chaos

I used to believe that if you just designed the right structure, good outcomes would follow. Boxes and arrows, reporting lines, clear mandates - surely that's all you need? Then I spent time inside actual institutions trying to govern emerging tech, and boy, did I get an education.

The FDA trying to regulate AI-powered medical devices. The FTC grappling with algorithmic discrimination. International bodies attempting to coordinate on AI governance while half the members couldn't define "neural network." It was like watching people try to build a plane while flying it, except the plane kept shape-shifting and occasionally tried to fly itself.

## The Dirty Secrets of Institutional Design

### Secret #1: The Org Chart is a Lie

Every institution has two structures:
1. The official one (what's on paper)
2. The real one (how things actually work)

I once mapped the real decision-making flow in a regulatory agency. It looked like someone threw spaghetti at a wall while drunk. The newest junior analyst had more influence than two department heads because she actually understood the technology and everyone secretly asked her what to do.

Lesson: Design for the informal networks, not just the formal structure.

### Secret #2: Culture Eats Strategy (and Structure) for Breakfast

You can design the most elegant institution in the world, but if the culture is "cover your ass and avoid hard decisions," that's what you'll get.

I've seen safety institutions with perfect mandates produce nothing but reports that say "more research is needed" because the culture rewarded not rocking the boat. Meanwhile, a scrappy team with no formal authority changed an entire industry's practices because their culture was "fix problems, ask permission later."

### Secret #3: Incentives Are Everything

Show me the incentives, and I'll show you the outcomes. Not the stated incentives - the real ones.

Real incentives I've seen:
- "Don't create headlines" (leads to: do nothing bold)
- "Maximize stakeholder happiness" (leads to: lowest common denominator)
- "Publish papers" (leads to: research theater)
- "Avoid lawsuits" (leads to: paralysis)
- "Look busy" (leads to: process for process's sake)

If you want an institution that actually ensures AI safety, you need incentives that reward actual safety improvements, even when they're inconvenient.

## Institution Types That Actually Work (Sometimes)

### The Startup Model: "Move Fast and Don't Break Humanity"

Small team, clear mission, high autonomy, iterative approach. Works great until:
- You need legitimacy (startups don't have subpoena power)
- You need to coordinate globally (good luck)
- You need to be around in 10 years (funding?)

Best for: Rapid response, innovation, proof of concept
Worst for: Enforcement, longevity, democratic accountability

### The Network Model: "Herding Cats, But Make It Official"

Loose coordination of existing entities. Like the Financial Stability Board but for AI. Works when:
- Members actually want to coordinate (rare)
- The problem is well-defined (lol)
- There's mutual benefit (sometimes)

I've seen this work for sharing threat intelligence. I've seen it fail spectacularly for setting standards because everyone wanted their standard to win.

### The Kitchen Sink Model: "We Do Everything!"

One mega-institution that handles all AI safety. Sounds efficient, is usually a disaster because:
- AI safety is too broad
- Expertise doesn't transfer (medical AI ‚â† military AI)
- Single point of failure
- Bureaucratic nightmare

The only time I've seen this work is when it's really multiple institutions pretending to be one for political reasons.

### The "Blessed Chaos" Model: Multiple Overlapping Institutions

Messy, inefficient, redundant... and surprisingly effective. Different institutions compete and collaborate, cover each other's gaps, provide checks and balances. It's ugly but antifragile.

## Designing Institutions That Humans Will Actually Use

### Start with the Verbs, Not the Nouns

Don't start with "we need a department of X." Start with:
- What needs to HAPPEN?
- Who needs to DO it?
- What would STOP them?
- What would HELP them?

Then build the minimal structure that enables those verbs.

### The Pizza Rule

If a decision-making body can't be fed with two pizzas, it's too big to make actual decisions. It's a committee that produces committees.

I've seen 50-person "decision-making bodies" that never made a decision harder than where to order lunch (and even that took three meetings).

### Design for Failure

Every institution will fail sometimes. Design for:
- Graceful degradation (what's the minimum viable function?)
- Fast recovery (how quickly can you fix mistakes?)
- Learning from failure (not just blame assignment)
- Redundancy (what's the backup plan?)

### The Antibody Problem

Institutions develop antibodies against change. Someone proposes something new? "That's not how we do things." "We tried that in 2019." "Legal will never approve."

Build in mechanisms to overcome institutional antibodies:
- Sunset clauses (force regular renewal)
- Innovation budgets (protected resources for trying new things)
- External rotation (bring in fresh blood)
- Failure amnesty (for good-faith experiments)

## Real Talk: The Human Side

### The Type of People You Need

**The True Believers**: Need some people who'll work nights and weekends because they believe in the mission. But not too many or you get zealotry.

**The Professionals**: Need some people who'll do good work regardless because that's what professionals do. But not too many or you get bureaucracy.

**The Bridge Builders**: Need people who can translate between technical and policy, between institution and outside world. These are gold.

**The Shit Umbrella**: Need leaders who'll protect the team from political rain while they do actual work. Rare but essential.

### The Type of People You'll Get

**The Climbers**: There for the resume line. Will leave after 18 months. Sometimes useful for their ambition.

**The Refugees**: Fleeing worse institutions. Often grateful and hardworking, sometimes too damaged by past dysfunction.

**The Theorists**: Love designing perfect systems, allergic to implementation. One is useful, more is paralysis.

**The Survivors**: Been there 20 years, know where bodies are buried. Either invaluable guides or immovable obstacles.

## My Institutional Design Checklist

When designing an AI safety institution, I ask:

1. **What's the smallest version that could work?** (Start small, grow deliberately)

2. **How will decisions actually get made?** (Not on paper - in reality)

3. **What happens when the first crisis hits?** (It will)

4. **How does it learn and change?** (Static = dead)

5. **Why would good people join?** (And stay?)

6. **What's the failure mode?** (How does it break?)

7. **Who's the natural enemy?** (Someone will oppose it)

8. **How does it die gracefully?** (All institutions should)

## Success Stories (Sort Of)

### The NTSB Model

Investigates transportation accidents. Why it works:
- Clear, narrow mandate
- Technical expertise respected
- Independence protected
- Recommendations, not regulations (others implement)
- Learning-focused, not blame-focused

Could this work for AI safety? Maybe for incident investigation...

### The Internet Engineering Task Force

Develops internet standards. Weird but works:
- "Rough consensus and running code"
- Anyone can participate
- Merit-based influence
- No formal authority but massive real impact

Lessons for AI: Technical communities can self-govern if incentives align.

### The Office That Saved the Ozone Layer

Small team at UNEP coordinated global response to ozone depletion. Worked because:
- Clear, measurable problem
- Technical solution existed
- Economic alternatives available
- Visible success built momentum

AI safety is messier, but the model of small, focused, empowered teams is instructive.

## The Uncomfortable Truths

**Truth 1**: The best institutional design can't overcome bad faith actors with power. If key stakeholders want it to fail, it will.

**Truth 2**: Legitimacy can't be designed, only earned. You need early wins that matter to real people.

**Truth 3**: The iron law of institutions: they become what they measure. Choose metrics wisely.

**Truth 4**: Every institution eventually becomes about preserving itself. Build in renewal or planned obsolescence.

**Truth 5**: The people matter more than the structure. A great team with a mediocre structure beats a mediocre team with a great structure every time.

## Your Mission, Should You Choose to Accept It

If you're designing AI safety institutions, remember:

- Start with minimum viable institution
- Build for adaptation, not perfection
- Protect the mission from the institution
- Measure what matters, not what's easy
- Plan for success AND failure
- Remember it's all about the humans

And please, for the love of all that is holy, don't create another 47-member international advisory committee that meets quarterly to produce non-binding recommendations that no one reads.

The world needs AI safety institutions that actually work. That means institutions designed for messy reality, not clean theory. Institutions that can act, learn, and adapt faster than the technology they're governing.

Now go build something that doesn't suck. The future is counting on it. üèóÔ∏è