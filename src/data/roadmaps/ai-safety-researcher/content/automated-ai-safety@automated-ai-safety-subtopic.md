
              <h2>Automated AI Safety Research</h2>
              <p>Leveraging AI systems to help solve AI safety problems.</p>
              
              <h3>Automation Opportunities</h3>
              <ul>
                <li><strong>Interpretability:</strong> Automated circuit discovery</li>
                <li><strong>Red Teaming:</strong> AI-generated adversarial prompts</li>
                <li><strong>Verification:</strong> Proof search and checking</li>
                <li><strong>Alignment:</strong> Automated oversight and feedback</li>
              </ul>
              
              <h3>Bootstrapping Safety</h3>
              <ul>
                <li>Using safer AI to build safer AI</li>
                <li>Recursive safety improvement</li>
                <li>Avoiding negative feedback loops</li>
                <li>Maintaining human oversight</li>
              </ul>
              
              <h3>Current Applications</h3>
              <ul>
                <li>LLM-assisted safety research</li>
                <li>Automated theorem proving</li>
                <li>Code generation for safety tools</li>
                <li>Literature review and synthesis</li>
              </ul>
              
              <h3>Risks and Considerations</h3>
              <ul>
                <li>Automating dangerous capabilities</li>
                <li>Over-reliance on AI judgment</li>
                <li>Maintaining research quality</li>
                <li>Preserving human understanding</li>
              </ul>
            