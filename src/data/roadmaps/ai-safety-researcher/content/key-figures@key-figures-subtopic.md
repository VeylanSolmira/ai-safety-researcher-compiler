# Key Figures in AI Safety

Understanding the key figures in AI safety helps you navigate the field's intellectual landscape, find mentors, and understand different research approaches. This guide covers the most influential researchers and their contributions.

## Core Leaders & Pioneers

### Stuart Russell
**Background**: UC Berkeley professor, AI textbook co-author  
**Key Contributions**:
- Author of "Human Compatible" - seminal book on AI alignment
- Pioneer of value alignment and assistance games
- Developed the "beneficial AI" framework
- Co-founded Center for Human-Compatible AI (CHAI)

**Why Important**: Russell brought academic rigor and mainstream credibility to AI safety concerns.

**Resources**:
- [@video@Stuart Russell on the AI Control Problem](https://www.youtube.com/watch?v=9i1WlcCudpU)
- [@article@Human Compatible book](https://humancompatible.ai/)
- [@course@CHAI research papers](https://humancompatible.ai/papers)

### Eliezer Yudkowsky
**Background**: MIRI founder, self-taught AI researcher  
**Key Contributions**:
- Founded Machine Intelligence Research Institute (MIRI)
- Developed early alignment theory concepts
- Wrote influential sequences on rationality and AI risk
- Pioneered many core AI safety concepts (mesa-optimization, corrigibility)

**Why Important**: Yudkowsky essentially founded the modern AI alignment field and shaped its core concerns.

**Resources**:
- [@article@The Sequences - Rationality: From AI to Zombies](https://www.lesswrong.com/rationality)
- [@video@AGI Safety from First Principles](https://www.youtube.com/watch?v=EUjc1WuyPT8)
- [@article@MIRI technical agenda](https://intelligence.org/research/)

### Paul Christiano
**Background**: Former OpenAI alignment lead, founded Alignment Research Center  
**Key Contributions**:
- Iterated Distillation and Amplification (IDA)
- Scalable oversight research
- AI safety via debate
- Eliciting Latent Knowledge (ELK) problem

**Why Important**: Christiano bridges theoretical alignment work with practical ML approaches.

**Resources**:
- [@article@AI Alignment blog posts](https://ai-alignment.com/)
- [@article@What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)
- [@video@Prosaic AI alignment talk](https://www.youtube.com/watch?v=fqhjAHRq_Lk)

### Dario Amodei
**Background**: Anthropic CEO, former VP of Research at OpenAI  
**Key Contributions**:
- Co-developed GPT-2 and GPT-3
- Constitutional AI (CAI) approach
- Responsible scaling policies
- AI safety via empirical research

**Why Important**: Leading practical implementation of safety techniques in frontier AI systems.

**Resources**:
- [@article@Constitutional AI paper](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback)
- [@article@Anthropic's Core Views on AI Safety](https://www.anthropic.com/index/core-views-on-ai-safety)
- [@video@Senate testimony on AI risks](https://www.youtube.com/watch?v=example)

### Geoffrey Hinton
**Background**: Turing Award winner, "Godfather of Deep Learning"  
**Key Contributions**:
- Pioneered backpropagation and deep learning
- Recently became vocal about AI existential risks
- Left Google to speak freely about AI dangers
- Advocates for international AI safety coordination

**Why Important**: His shift from AI development to safety advocacy legitimized concerns in mainstream ML.

**Resources**:
- [@video@60 Minutes interview on AI risks](https://www.youtube.com/watch?v=example)
- [@article@Why I quit Google - AI risk statement](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html)

## Technical Research Leaders

### Chris Olah
**Background**: Co-founded Anthropic, formerly at OpenAI and Google Brain  
**Key Contributions**:
- Pioneer of neural network interpretability
- Created influential visualizations of neural networks
- Developed circuits approach to interpretability
- Founded Distill journal for ML clarity

**Why Important**: Made interpretability research accessible and inspired a generation of researchers.

**Resources**:
- [@article@Distill.pub publications](https://distill.pub/)
- [@article@Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)
- [@article@Thread on interpretability](https://twitter.com/ch402)

### Jan Leike
**Background**: Leads Superalignment team at OpenAI, formerly DeepMind  
**Key Contributions**:
- Recursive reward modeling
- AI safety via debate
- Scalable alignment research
- Building aligned AI assistants

**Why Important**: Leading practical efforts to align superhuman AI systems.

**Resources**:
- [@article@OpenAI Superalignment announcement](https://openai.com/blog/introducing-superalignment)
- [@article@Alignment research papers](https://jan.leike.name/)

### Victoria Krakovna
**Background**: DeepMind research scientist, AI safety team  
**Key Contributions**:
- Specification gaming examples database
- Impact measures for AI safety
- Cooperative AI research
- Side effects and negative externalities

**Why Important**: Bridges theoretical safety with practical ML engineering at DeepMind.

**Resources**:
- [@article@Specification gaming examples](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/)
- [@article@DeepMind safety publications](https://deepmind.com/safety)

## Governance & Policy Leaders

### Helen Toner
**Background**: Georgetown CSET, former OpenAI board member  
**Key Contributions**:
- AI governance research
- China AI policy expertise
- International AI cooperation
- AI safety standards development

**Why Important**: Key figure in translating technical safety concerns into policy.

**Resources**:
- [@article@CSET publications](https://cset.georgetown.edu/publications/)
- [@article@AI governance frameworks](https://example.com)

### Allan Dafoe
**Background**: DeepMind governance lead, Oxford professor  
**Key Contributions**:
- AI governance research agenda
- International AI cooperation
- Long-term AI policy
- Cooperative AI initiatives

**Why Important**: Leading efforts on international coordination for AI safety.

**Resources**:
- [@article@AI Governance: A Research Agenda](https://www.allandafoe.com/governance)
- [@article@DeepMind governance team work](https://deepmind.com/safety)

## Rising Stars & Specialized Researchers

### Mechanistic Interpretability Leaders
- **Anthropic Interpretability Team**: Tom Brown, Jared Kaplan, Catherine Olsson
- **Redwood Research**: Buck Shlegeris, team focusing on interpretability and robustness
- **Independent researchers**: Many talented researchers working on specific interpretability problems

### Other Notable Figures
- **Ajeya Cotra**: Open Philanthropy, AI timelines and impact assessment
- **Evan Hubinger**: Mesa-optimization, inner alignment problems
- **Richard Ngo**: DeepMind, agent foundations
- **Rohin Shah**: UC Berkeley, AI alignment newsletter curator

## How to Follow the Field

### Key Resources
1. **Alignment Forum**: Central hub for technical alignment research
2. **LessWrong**: Broader rationality and AI safety community
3. **AI Safety Newsletter**: Weekly updates by Rohin Shah
4. **Twitter/X**: Many researchers share updates (@paulfchristiano, @ESYudkowsky, etc.)

### Research Groups to Follow
- MIRI (Machine Intelligence Research Institute)
- Anthropic
- OpenAI Safety Team
- DeepMind Safety Team
- Redwood Research
- CHAI (Center for Human-Compatible AI)
- FHI (Future of Humanity Institute)
- CAIS (Center for AI Safety)

## Career Advice

### Learning Paths
1. **Technical route**: Study their papers, implement their ideas, contribute to their research
2. **Governance route**: Understand their policy proposals, work on implementation
3. **Communication route**: Help translate their work for broader audiences

### Getting Involved
- Attend their talks and workshops
- Contribute to their open source projects
- Apply for positions at their organizations
- Engage thoughtfully with their work online

Remember: The field is collaborative. These researchers actively mentor newcomers and share knowledge openly. Don't hesitate to engage respectfully with their work.