# Foundational Papers

Essential papers that every AI safety researcher should understand deeply. These papers form the technical and conceptual foundation of the field.

## 1. "Attention Is All You Need" (Vaswani et al., 2017)

**Why**: Understanding transformers is non-negotiable for modern AI safety  
**Key concepts**: Attention mechanisms, model architecture, scaling properties  
**Safety relevance**: Interpretability, alignment techniques, capability understanding

Visit the following resources to learn more:

- [@article@Attention Is All You Need (Original Paper)](https://arxiv.org/abs/1706.03762)
- [@article@The Illustrated Transformer (Visual Guide)](https://jalammar.github.io/illustrated-transformer/)
- [@video@Transformer Neural Networks Explained (YouTube)](https://www.youtube.com/watch?v=4Bdc55j80l8)

## 2. "Concrete Problems in AI Safety" (Amodei et al., 2016)

**Why**: Still the clearest articulation of the core technical safety challenges  
**Key concepts**: Reward misspecification, safe exploration, robustness, interpretability, distributional shift  
**Safety relevance**: Defines the problem space that most current work addresses

Visit the following resources to learn more:

- [@article@Concrete Problems in AI Safety (Original Paper)](https://arxiv.org/abs/1606.06565)
- [@article@DeepMind's Summary of Concrete Problems](https://deepmindsafetyresearch.medium.com/concrete-problems-in-ai-safety-problems-6c7a9e6f2c2c)
- [@video@Concrete Problems in AI Safety Explained](https://www.youtube.com/watch?v=AjyM-f8rDpg)

## 3. "Training Language Models to Follow Instructions with Human Feedback" (Ouyang et al., 2022)

**Why**: RLHF is currently the dominant alignment technique in deployment  
**Key concepts**: Human preference learning, reward modeling, policy optimization  
**Safety relevance**: Practical alignment implementation, current best practices

Visit the following resources to learn more:

- [@article@InstructGPT Paper (Original)](https://arxiv.org/abs/2203.02155)
- [@article@OpenAI's Blog Post on InstructGPT](https://openai.com/research/instruction-following)
- [@article@Understanding RLHF (Hugging Face)](https://huggingface.co/blog/rlhf)
- [@video@RLHF Explained Simply](https://www.youtube.com/watch?v=2MBJOuVq380)

## Additional Essential Papers

Consider also reading:

- [@article@Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
- [@article@Risks from Learned Optimization](https://arxiv.org/abs/1906.01820)
- [@article@Alignment Research Center's Core Views](https://alignment.org/core-views/)
- [@article@Superintelligence: Paths, Dangers, Strategies (Book)](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834)