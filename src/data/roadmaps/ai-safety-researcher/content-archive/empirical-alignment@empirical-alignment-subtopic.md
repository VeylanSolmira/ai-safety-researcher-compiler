
              <h2>Empirical Alignment Research</h2>
              <p>Hands-on implementation and testing of state-of-the-art alignment techniques.</p>
              
              <h3>RLHF (Reinforcement Learning from Human Feedback)</h3>
              <ul>
                <li><strong>Supervised Fine-tuning:</strong> Initial behavior cloning from demonstrations</li>
                <li><strong>Reward Model Training:</strong> Learning human preferences from comparisons</li>
                <li><strong>PPO Optimization:</strong> Reinforcement learning against reward model</li>
                <li><strong>KL Penalties:</strong> Preventing mode collapse and maintaining diversity</li>
              </ul>
              
              <h3>Constitutional AI</h3>
              <ul>
                <li><strong>Principle-Based Training:</strong> Encoding values as constitutional principles</li>
                <li><strong>Self-Critique:</strong> Models evaluate their own outputs</li>
                <li><strong>Revision Training:</strong> Learning to improve based on critiques</li>
                <li><strong>Reduced Human Feedback:</strong> Scaling oversight through AI assistance</li>
              </ul>
              
              <h3>Advanced Techniques</h3>
              <ul>
                <li>Direct Preference Optimization (DPO)</li>
                <li>Instruction Following through FLAN/InstructGPT</li>
                <li>Safety-specific fine-tuning approaches</li>
                <li>Multi-objective alignment methods</li>
              </ul>
              
              <h3>Experimental Methodology</h3>
              <ul>
                <li>Benchmark design and evaluation</li>
                <li>A/B testing alignment techniques</li>
                <li>Red teaming aligned models</li>
                <li>Long-term behavior analysis</li>
              </ul>
            