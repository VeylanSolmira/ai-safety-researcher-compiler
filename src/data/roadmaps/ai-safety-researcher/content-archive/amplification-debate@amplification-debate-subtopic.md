
              <h2>Scalable Oversight Techniques</h2>
              <p>As AI systems become more capable, we need oversight methods that scale beyond human ability to directly evaluate outputs.</p>
              
              <h3>Iterated Amplification (IDA)</h3>
              <ul>
                <li><strong>Core Idea:</strong> Use AI assistance to amplify human oversight</li>
                <li><strong>Process:</strong> Human + AI system supervises training of new AI</li>
                <li><strong>Iteration:</strong> Each generation helps train the next</li>
                <li><strong>Goal:</strong> Maintain alignment while increasing capability</li>
              </ul>
              
              <h3>AI Safety via Debate</h3>
              <ul>
                <li><strong>Adversarial Setup:</strong> Two AIs debate, human judges</li>
                <li><strong>Truth-Seeking:</strong> Incentive to expose opponent's errors</li>
                <li><strong>Scalability:</strong> Humans can judge debates on complex topics</li>
                <li><strong>Limitations:</strong> Assumes truth has natural advantage</li>
              </ul>
              
              <h3>Recursive Reward Modeling</h3>
              <ul>
                <li>Use AI to help evaluate AI behavior</li>
                <li>Break complex tasks into simpler pieces</li>
                <li>Maintain human oversight at each level</li>
                <li>Scale to superhuman performance safely</li>
              </ul>
              
              <h3>Challenges and Open Questions</h3>
              <ul>
                <li>Preserving alignment through amplification</li>
                <li>Detecting manipulation in debates</li>
                <li>Computational complexity</li>
                <li>Philosophical questions about truth and judgment</li>
              </ul>
            